{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ba8269-05a0-4696-9418-b3db0cc82b77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/exp6/weights/hubconf.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# YOLOv5 PyTorch HUB Inference (DetectionModels only)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/exp6/weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# yolov5n - yolov5x6 or custom\u001b[39;00m\n\u001b[1;32m      5\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/rf_data/result_frame_138847889935637681_bw_25E+6 (1).png\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# file, Path, PIL.Image, OpenCV, nparray, list\u001b[39;00m\n\u001b[1;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m model(im)  \u001b[38;5;66;03m# inference\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:584\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[1;32m    583\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[0;32m--> 584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m    587\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:98\u001b[0m, in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     96\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:844\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:980\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1037\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/exp6/weights/hubconf.py'"
     ]
    }
   ],
   "source": [
    "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('/home/jovyan/exp6/weights', 'best', force_reload=True, source = 'local')  # yolov5n - yolov5x6 or custom\n",
    "im = r'/home/jovyan/rf_data/result_frame_138847889935637681_bw_25E+6 (1).png'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
    "results = model(im)  # inference\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f112461-1bc4-432f-a5b3-190fbb570384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from RFclassification.data_utils.scaling import Scaling\n",
    "import glob\n",
    "import cv2\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from yolov5.utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from yolov5.utils.plots import Annotator, colors, save_one_box\n",
    "from yolov5.utils.torch_utils import select_device, smart_inference_mode\n",
    "\n",
    "class LoadImagesNew(LoadImages):\n",
    "    def __init__(self, path, img_size=640, stride=32, auto=True, transforms=None, vid_stride=1):\n",
    "        super().__init__(path, img_size=640, stride=32, auto=True, transforms=None, vid_stride=1)\n",
    "\n",
    "class Detection():\n",
    "    def __init__(self):\n",
    "        self.yaml_file = '/home/jovyan/yolov5/transfer_learning/spectrogram_test.yaml'\n",
    "        \n",
    "    def obtain_np_array(self, path : str = r'/home/jovyan/rf_data/result_frame_138847889935637681_bw_25E+6 (1).png') -> np.array:\n",
    "        self.img = cv2.imread(path) #this is im0\n",
    "        \n",
    "        \n",
    "        return self.img\n",
    "    def split(self, img : np.array) -> list\n",
    "        self.lst = []\n",
    "        \n",
    "        self.dimensions = img.shape\n",
    "\n",
    "        # (Number of blocks, excess)\n",
    "        self.slice_info = (\n",
    "            math.floor(self.dimensions[1] / self.dimensions[0]),\n",
    "            self.dimensions[1] % self.dimensions[0],\n",
    "        )\n",
    "        \n",
    "        self.block_count = self.slice_info[0] + int(self.slice_info[1] != 0)\n",
    "        \n",
    "        h = self.dimensions[0]\n",
    "        l = self.dimensions[1]\n",
    "\n",
    "        # Slice for the first n blocks\n",
    "        for block in range(self.slice_info[0]):\n",
    "            self.lst.append(img[:, block * h : (block + 1) * h, :])\n",
    "\n",
    "        # Slice for the last block (excess)\n",
    "        if self.slice_info[1]:\n",
    "            self.lst.append(\n",
    "                img[\n",
    "                    :,\n",
    "                    self.slice_info[0] * h : self.slice_info[0] * h\n",
    "                    + self.slice_info[1]\n",
    "                    - 1,\n",
    "                    :,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return self.lst\n",
    "    \n",
    "    def model_insert(self, imgs : list, model_path):\n",
    "        '''\n",
    "        Insert the imgs as a list of np.arrays\n",
    "        Even if there is only 1 img, put the np.array inside a list first\n",
    "        '''\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Initialization of model\n",
    "        device = select_device(device)\n",
    "        model = DetectMultiBackend(model_path, device=device, dnn=False, \n",
    "                                   data=self.yaml_file, fp16=False)\n",
    "        stride, names, pt = model.stride, model.names, model.pt\n",
    "        imgsz = check_img_size((640, 640), s=stride)  # check image size\n",
    "        \n",
    "        batch_size = 1\n",
    "        \n",
    "        for img in imgs:\n",
    "            im0 = img\n",
    "            # Need to dig out what is img_size, stride and auto\n",
    "            im = letterbox(im0, self.img_size, stride=self.stride, auto=self.auto)[0]  # padded resize\n",
    "            im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "            im = np.ascontiguousarray(im)  # contiguous\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235aaa9d-8fe8-41f4-8741-4f2cdea3c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/jovyan/exp6/weights/best.pt'\n",
    "dataset_path = '/home/jovyan/rf_data/'\n",
    "\n",
    "# The image will be segmented for better accuracy. \n",
    "# The image will later be combined back to the original size\n",
    "scaling = True\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
